---
layout: post
title: 线上应用服务一次内存泄露的排查
comments: true
categories: java
---
背景：云打印服务api，springboot开发。

线上打印服务的现象：内存会持续上升，当上升当容器的限制内存时，期间接口会出现超时的情况，内存上升到容器限制值时服务重启。
超时不是所有的接口，qps40左右，会有5个左右请求超时，从日志上看其它请求响应时间正常。

#排查思路。

排查中所需要使用的工具：<br>
jstack：jdk自带的线程堆栈分析工具<br>
jmap：较多的功能，可以打印dump文件，查看堆中的对象信息命令，ClassLoader，finalizer等信息。<br>
jstat：可以查看jvm的gc情况<br>



1. 查看占用内存高的线程。


    top -Hp 7
    
现象：其中会出现几个线程内存和cpu突然的升高。

2. 继续排查，想通过jstack打印出当前pid的线程ID、线程的状态（wait、sleep、running 等状态），确认下是否是代码中逻辑导致的死循环之类


    "qtp397857830-13" #13 prio=5 os_prio=0 tid=0x00007f6cb9653000 nid=0x1d runnable [0x00007f6c8e875000]
       java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000006c8d2ac20> (a sun.nio.ch.Util$3)
        - locked <0x00000006c8d2ac10> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000006c8d2a9e8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:422)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:359)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:357)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:181)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
        at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
        at java.lang.Thread.run(Thread.java:745)


    "qtp397857830-19" #19 prio=5 os_prio=0 tid=0x00007f6cb965c000 nid=0x23 waiting on condition [0x00007f6c8c57e000]
       java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000006c82a46a0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:656)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:49)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:720)
        at java.lang.Thread.run(Thread.java:745)

有等待线程和运行线程，还有一些gc线程等，看不出有什么问题。

3. 再打印出gc信息分析

这里发现一个现象，fgc的频率非常频繁，而且每次ygc之后都有幸存区的内存到老年代。

![vm-mem](https://raw.githubusercontent.com/zhoufeilongjava/markdownPictures/master/github/githubPages/vm-mem.jpg)

这里再引用一张内存增长和堆内存变化曲线图。

当时使用jmap直接dump堆文件（这里不要学，线上dump会导致jvm暂停），导入到visualVM分析，发现java.lang.ref.Finalizer中有大量的对象，基本可以确定这里是内存的泄漏点。
沿着其中的对象很明显的可以看见有大量的ScriptEngine调用动态语言的字符串对象，由此怀疑是ScriptEngine处有内存泄漏。
这里当时的写法是每次调用都会eval，然后再invokeFunction，等于是每次预编译后再调用方法。

这里继续搜索相关资料，发现是jdk的bug  https://bugs.openjdk.java.net/browse/JDK-8197544?spm=a2c4e.10696291.0.0.779619a4ZA7Z73

### 优化方案：

更改代码，在静态代码块中eval，后续的调用直接是invokeFunction，再进行压测，发现内存泄漏问题已经没有了。


### 结尾

这里还需要考虑调整s0，s1区，伊甸园区的大小，因为项目的特点就是会有大量的比较大的字符串对象产生，可能会有年轻代内存不够，导致对象进入老年代的可能，
这里后续还需要调优，因为目前还有一点内存略微上升的情况存在。

通过java.lang.ref.Finalizer来查找等待回收的资源。